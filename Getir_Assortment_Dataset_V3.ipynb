{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e958abad",
   "metadata": {},
   "source": [
    "#Hard coded values\n",
    "Column names\n",
    "'createdatl,'updatedatl','marketproduct_id','Eddress ID','product_barcode','marketproduct_name_de','segment_name',\n",
    "'brand_name','category_name_en','subcategory_name_en','supplier_names','list_price,'g10_price','transfer_groups','isvisible'\n",
    "\n",
    "Segment filter\n",
    "\"Asset\", \"Delist\", \"None\", \"Test\", \"Only Promo\"\n",
    "\n",
    "category managers\n",
    "\"Marius\",\"Fabian\",\"Gina\",\"Hamid\",\"Amadeus\",\"Rozan\",\"Francisco\",\"Eike\",\"Evrim\",\"Leonard\",\"Andrea\"\n",
    "\n",
    "Cities\n",
    "\"Berlin\",\"Berlin Mini\", \"Dresden\",\"Leipzig\",\"Hamburg\",\"Bremen\",\"Hannover\",\"Düsseldorf\",\"Dortmund\",\"Köln\",\"Bonn\",\"Essen\",\"München\", \"Nürnberg/Fürth\",\"Frankfurt\",\"Offenbach\",\"Stuttgart\",\"Heidelberg\",\"Mannheim\",\"Karlsruhe\",\"Darmstadt\",\"Augsburg\"\n",
    "\n",
    "segments\tsegment_name\n",
    "1\tTest\n",
    "10\tSeasonal\n",
    "11\tSupply Shortage\n",
    "12\tStand By\n",
    "2\tLaunch\n",
    "3\tRegular\n",
    "4\tRegional\n",
    "5\tOnly Promo\n",
    "6\tDelist On Depletion\n",
    "7\tDelist\n",
    "8\tAsset\n",
    "\n",
    "\n",
    "final output columns format\n",
    "\n",
    "\n",
    "#Using Instructions\n",
    "1 - save panel report on desktop and rename until date. (ex - G10andGetirBuyuk_AllProducts_without_TRDE_20230605.xlsx)\n",
    "2 - save eddress export all on desktop and rename as Inventory.xls\n",
    "3 - run the code - you will have a sheet on your desktop \"New_assortment\"\n",
    "4 - copy the data from \"New_assortment\" and paste it in Assortment list sheet - \"Weekly updated Assortment List\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3442d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/swaroopbasavarajmusti/Desktop/G10_and_GB_purchaseorderInfo_20230731.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#------------------------------\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# read purchase order report \u001b[39;00m\n\u001b[1;32m     57\u001b[0m Path_C \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/swaroopbasavarajmusti/Desktop/G10_and_GB_purchaseorderInfo_20230731.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 58\u001b[0m PO \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath_C\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#rename primary keys and reduce to only required columns\u001b[39;00m\n\u001b[1;32m     61\u001b[0m PO\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItem\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetir Product Id\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/swaroopbasavarajmusti/Desktop/G10_and_GB_purchaseorderInfo_20230731.xlsx'"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from datetime import date\n",
    "today = dt.now()\n",
    "mask = '%Y%m%d'\n",
    "dte = dt.now().strftime(mask)\n",
    "\n",
    "#Read the downloaded report\n",
    "Path_A = \"/Users/swaroopbasavarajmusti/Desktop/G10andGetirBuyuk_AllProducts_without_TRDE.xlsx\"\n",
    "\n",
    "Assortment = pd.read_excel(Path_A)\n",
    "\n",
    "#change the marketplace product id column name (not required but to avoid confusion)\n",
    "\n",
    "Assortment.rename(columns={'marketproduct_id':'Getir Product Id'}, inplace=True)\n",
    "\n",
    "#select only required columns\n",
    "\n",
    "Assortment = Assortment[['createdatl','updatedatl','Getir Product Id','product_barcode','marketproduct_name_de','segment_name',\n",
    "'brand_name','category_name_en','subcategory_name_en','supplier_names','net_net_buying_price_with_vat','g10_price','transfer_groups','isvisible',\\\n",
    "'isbundle','exp_days_lifetime','exp_days_allowed','exp_days_warning','exp_days_dead','inbox_quantitiy',\\\n",
    "'palet_inbox_quantity','storagetype','transfer_coli_count','is_picked_to_zero','market_status']]\n",
    "\n",
    "#filter unwanted segments from the dataframe\n",
    "Segment_filters = [\"Asset\", \"None\", \"Test\", \"Only Promo\"]\n",
    "Assortment = Assortment[~Assortment.segment_name.isin(Segment_filters)].reset_index(drop = True)\n",
    "\n",
    "#sort the values based on the date created\n",
    "Assortment = Assortment.sort_values(by=\"createdatl\",ascending = False).reset_index(drop = True)\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#creating dictionary to match the product ids\n",
    "\n",
    "Path_B = \"/Users/swaroopbasavarajmusti/Desktop/Inventory.xls\"\n",
    "Dictionary = pd.read_excel(Path_B)\n",
    "\n",
    "Dictionary = Dictionary[Dictionary['Getir Product Id'].notna()].reset_index(drop = True)\n",
    "Dictionary = Dictionary[['Getir Product Id','Product Id','Product SKU','Enabled / Disabled']]\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#left joining Panel assortment with eddress ID dictionary\n",
    "\n",
    "Assortment_Inventory = pd.merge(Assortment, \n",
    "                      Dictionary, \n",
    "                      on ='Getir Product Id', \n",
    "                      how ='left')\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "# read purchase order report \n",
    "\n",
    "Path_C = \"/Users/swaroopbasavarajmusti/Desktop/G10_and_GB_purchaseorderInfo_20230731.xlsx\"\n",
    "PO = pd.read_excel(Path_C)\n",
    "\n",
    "#rename primary keys and reduce to only required columns\n",
    "PO.rename(columns={'Item':'Getir Product Id'}, inplace=True)\n",
    "PO.rename(columns={'Job Status Name':'Job_status'}, inplace=True)\n",
    "\n",
    "#filter unwanted columns from the dataframe and create only required dataframe\n",
    "PO = PO[['Getir Product Id','Purchaseorder Createdat L','Job_status']]\n",
    "\n",
    "#filter unwanted job status from the dataframe and create only required dataframe\n",
    "Job_Status_Name_filters = [\"CANCELLED\"]\n",
    "PO = PO[~PO.Job_status.isin(Job_Status_Name_filters)].reset_index(drop = True)\n",
    "\n",
    "#look for unique values of products ordered and create ordered status\n",
    "Ordered_products = pd.DataFrame(data=PO['Getir Product Id'].unique(), columns=[\"Getir Product Id\"])\n",
    "Ordered_products['Order_Status'] = 'TRUE'\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#join the two table with primary\n",
    "Assortment_Joined = pd.merge(Assortment_Inventory, \n",
    "                      Ordered_products, \n",
    "                      on ='Getir Product Id', \n",
    "                      how ='left')\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#filtering only required columns\n",
    "Assortment_Joined = Assortment_Joined[['createdatl','updatedatl','Getir Product Id','Product Id','Product SKU','product_barcode','marketproduct_name_de','segment_name',\n",
    "'brand_name','category_name_en','subcategory_name_en','supplier_names','net_net_buying_price_with_vat','g10_price','isvisible','transfer_groups','Enabled / Disabled','Order_Status',\\\n",
    "'isbundle','exp_days_lifetime','exp_days_allowed','exp_days_warning','exp_days_dead','inbox_quantitiy',\\\n",
    "'palet_inbox_quantity','storagetype','transfer_coli_count','is_picked_to_zero','market_status']]\n",
    "\n",
    "#convert datetime format to just date\n",
    "Assortment_Joined['createdatl'] = Assortment_Joined['createdatl'].dt.date\n",
    "Assortment_Joined['updatedatl'] = Assortment_Joined['updatedatl'].dt.date\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#create categories and categories manager dictionary\n",
    "Category_dictionary = {\\\n",
    "\"Rozan\":['Coffee & Tea'],\n",
    "\"Marius\":['Baby','Beer','Home Care','Personal Care','Pets','Spirits & Co.','Wine & Co.'],\n",
    "\"Leonard\":['Fruit & Veg'],\n",
    "\"Hamid\":['Cheese & Co.','Milk & Eggs','Yoghurt & Co.','Cold Cuts','Meat & Fish'],\n",
    "\"Francisco\":['Fresh & Ready'],\n",
    "\"Fabian\":['Bakery'],\n",
    "\"Evrim\":['Frozen Food','Ice Cream'],\n",
    "\"Eike\":['Drinks','Breakfast','Pantry','Pasta & Rice','Canned','Crisps & Snacks'],\n",
    "\"Amadeus\":['Christmas','Easter','Sweet Snacks'],\n",
    "\"Other\":['0','DUMMY TEST KATEGORI GE',\"Mother's Day\",'Top Deals','Earthquake Aid','Dairy','Cheese & Deli','Vegan & Veggie','Confectionery','Vegan']}\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#match the categories and get the name of category manager\n",
    "for ind, row in Assortment_Joined.iterrows():\n",
    "    if Assortment_Joined.loc[ind, 'category_name_en'] == \"\":\n",
    "        Assortment_Joined.loc[ind, 'Buyer'] = \"Not Assigned\"\n",
    "    else:\n",
    "        Category = Assortment_Joined.loc[ind, 'category_name_en']\n",
    "        Assortment_Joined.loc[ind, 'Buyer'] = [k for k, v in Category_dictionary.items() if Category in v]\n",
    "\n",
    "\n",
    "# #get only the name without brackets\n",
    "#Assortment_Joined['Buyer'] = ''\n",
    "Assortment_Joined['Buyer'] = Assortment_Joined['Buyer'].astype(str)\n",
    "Assortment_Joined['Buyer'] = Assortment_Joined['Buyer'].str[2:-2]\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#change the eddress enabled/disabled status to true and false\n",
    "for ind,row in Assortment_Joined.iterrows():\n",
    "    if Assortment_Joined.loc[ind, 'Enabled / Disabled'] == 'Enabled':\n",
    "        Assortment_Joined.loc[ind, 'Live Gorillas app'] = \"TRUE\"\n",
    "    else:\n",
    "        Assortment_Joined.loc[ind, 'Live Gorillas app'] = \"FALSE\"\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#Generate the y/n combination for transfer group(mfc) checks\n",
    "Assortment_Joined['transfer_groups'] = Assortment_Joined['transfer_groups'].astype(str)\n",
    "\n",
    "#create a list of cities \n",
    "cities = [\"Berlin\",\"Berlin mini\", \"Dresden\",\"Leipzig\",\"Hamburg\",\"Bremen\",\\\n",
    "          \"Hannover\",\"Düsseldorf\",\"Dortmund\",\"Köln\",\"Bonn\",\"Essen\",\"München\",\\\n",
    "          \"NürnbergFürth\",\"Frankfurt\",\"Offenbach\",\"Stuttgart\",\"Heidelberg\",\\\n",
    "          \"Mannheim\",\"Karlsruhe\",\"Darmstadt\",\"Augsburg\"]\n",
    "\n",
    "Assortment_Joined['Cities'] = ''\n",
    "#iterate over transfer groups to check for cities\n",
    "for c in cities:\n",
    "    for ind, row in Assortment_Joined.iterrows():\n",
    "        x = \"merged \"+c\n",
    "        if c in Assortment_Joined.loc[ind, 'transfer_groups']:\n",
    "            Assortment_Joined.loc[ind, c] = \"Y\"\n",
    "            a = Assortment_Joined.loc[ind, 'Cities']\n",
    "            b = c\n",
    "            new_Cit = a+\", \"+b\n",
    "            Assortment_Joined.loc[ind, 'Cities'] = new_Cit\n",
    "        else:\n",
    "            Assortment_Joined.loc[ind, c] = \"N\"\n",
    "            \n",
    "Assortment_Joined['Cities'] = Assortment_Joined['Cities'].str[2:]\n",
    "#------------------------------\n",
    "\n",
    "#renaming and del column\n",
    "Assortment_Joined.rename(columns={'createdatl':'Product Card Creation Date'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'updatedatl':'Update date'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'net_net_buying_price_with_vat':'Buying Price (w VAT)'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'g10_price':'Selling Price (w VAT)'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'isvisible':'Live Getir app'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'Product Id':'Eddress Product Id'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'product_barcode':'GTIN_Panel'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'Product SKU':'GTIN_Eddress'}, inplace=True)\n",
    "Assortment_Joined.rename(columns={'segment_name':'Segment_Panel'}, inplace=True)\n",
    "\n",
    "#------------------------------\n",
    "#additional formatting\n",
    "Assortment_Joined['Update date'] = ''\n",
    "del Assortment_Joined['transfer_groups']\n",
    "Assortment_Joined['Order_Status'] = Assortment_Joined['Order_Status'].fillna('FALSE')\n",
    "del Assortment_Joined['Enabled / Disabled']\n",
    "        \n",
    "#------------------------------\n",
    "\n",
    "#sorting and reset index\n",
    "Assortment_Joined.sort_values(by='Product Card Creation Date', ascending = False ,inplace = True)\n",
    "Assortment_Joined = Assortment_Joined.reset_index(drop=True)\n",
    "\n",
    "#-------------------------------\n",
    "\n",
    "#adding mdm data to dataset to get the product status on mdm\n",
    "\n",
    "Path_D = \"/Users/swaroopbasavarajmusti/Desktop/mdm_data.xlsm\"\n",
    "mdm_data_all = pd.read_excel(Path_D,'Entities')\n",
    "mdm_data_all = mdm_data_all.rename(columns=mdm_data_all.iloc[0])\n",
    "mdm_data_all = mdm_data_all.iloc[1:].reset_index(drop = True)\n",
    "mdm_data = mdm_data_all.copy()\n",
    "mdm_data = mdm_data[['Eddress Product ID','Web Pages.File Name','Product Status']]\n",
    "mdm_data = mdm_data[mdm_data['Eddress Product ID'].notna()].reset_index(drop =True)\n",
    "mdm_data.rename(columns={'Eddress Product ID':'Eddress Product Id'}, inplace=True)\n",
    "mdm_data.rename(columns={'Product Status':'Segment_MDM'}, inplace=True)\n",
    "mdm_data = mdm_data.drop_duplicates(subset=['Eddress Product Id'], keep='first')\n",
    "check1 = mdm_data.copy()\n",
    "\n",
    "Merged_assortment = pd.merge(Assortment_Joined, \n",
    "                      mdm_data, \n",
    "                      on ='Eddress Product Id', \n",
    "                      how ='left')\n",
    "\n",
    "#-------------------------------\n",
    "#checking if the product is created on eddress by creating a column\n",
    "\n",
    "for ind,row in Merged_assortment.iterrows():\n",
    "    if pd.isnull(Merged_assortment.loc[ind, 'Eddress Product Id']) == True:\n",
    "        Merged_assortment.loc[ind, 'Created on Eddress'] = \"FALSE\"\n",
    "    else:\n",
    "        Merged_assortment.loc[ind, 'Created on Eddress'] = \"TRUE\"\n",
    "\n",
    "#-------------------------------\n",
    "\n",
    "#create segment dictionary and segment attributes to sync statuses\n",
    "Segment_Dictionary = {\\\n",
    "\n",
    "\"PUBLISHED\":['Regular','Regional'],\n",
    "\"UNPUBLISHED - New Ingoing\":['Launch'],\n",
    "\"PUBLISHED - Deplete\":['Seasonal','Only Promo','Delist On Depletion','Consumable'],\n",
    "\"UNAVAILABLE - OOS Supplier\":['Stand By','Supply Shortage'],\n",
    "\"UNPUBLISHED - Delisted\":['Delist','None','Asset','Test']\n",
    "\n",
    "}\n",
    "\n",
    "#create new segment statuts based on dictionary\n",
    "for ind, row in Merged_assortment.iterrows():\n",
    "    if pd.isnull(Merged_assortment.loc[ind, 'Segment_Panel']) == True:\n",
    "        Merged_assortment.loc[ind, 'Segment_New'] = \"\"\n",
    "    else:\n",
    "        segment = str(Merged_assortment.loc[ind, 'Segment_Panel'])\n",
    "        Merged_assortment.loc[ind, 'Segment_New'] = [k for k, v in Segment_Dictionary.items() if segment in v]\n",
    "\n",
    "#get only the Segments without brackets\n",
    "Merged_assortment['Segment_New'] = Merged_assortment['Segment_New'].astype(str)\n",
    "Merged_assortment['Segment_New'] = Merged_assortment['Segment_New'].str[2:-2]\n",
    "\n",
    "#create a filter column to filter products that have to be updated\n",
    "for ind, row in Merged_assortment.iterrows():\n",
    "    if pd.isnull(Merged_assortment.loc[ind, 'Segment_MDM']) == True:\n",
    "        Merged_assortment.loc[ind, 'Segment_update'] = \"\"\n",
    "    elif Merged_assortment.loc[ind, 'Segment_New'] == Merged_assortment.loc[ind, 'Segment_MDM']:\n",
    "        Merged_assortment.loc[ind, 'Segment_update'] = False\n",
    "    else:\n",
    "        Merged_assortment.loc[ind, 'Segment_update'] = True\n",
    "\n",
    "#-------------------------------\n",
    "#vhange price volumns to euro currency format\n",
    "\n",
    "Merged_assortment[\"Buying Price (w VAT)\"] = Merged_assortment[\"Buying Price (w VAT)\"].map(\"€ {:_.2f}\".format).str.translate(str.maketrans(\"_.\", \".,\"))\n",
    "Merged_assortment[\"Selling Price (w VAT)\"] = Merged_assortment[\"Selling Price (w VAT)\"].map(\"€ {:_.2f}\".format).str.translate(str.maketrans(\"_.\", \".,\"))\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "#bring relistings and delistings dataframe and add it to main df\n",
    "#path and load from desktop\n",
    "Path_f = '/Users/swaroopbasavarajmusti/Desktop/productCardChange.xlsx'\n",
    "metadata = pd.read_excel(Path_f)\n",
    "today = date.today()\n",
    "\n",
    "#filter for delist on depletion product with ids\n",
    "Dod = metadata.copy()\n",
    "Dod = Dod.loc[Dod['updated_field'] == 'segments']\n",
    "Dod = Dod.sort_values(by=\"updated_at\",ascending = False)\n",
    "Dod = Dod.drop_duplicates(subset=['product_id'], keep='first')\n",
    "Dod = Dod.loc[Dod['new_value'] == 6]\n",
    "Dod['updated_at'] = Dod['updated_at'].dt.date\n",
    "Dod['Date_filter'] = today - Dod['updated_at']\n",
    "Dod['Date_filter'] = pd.to_timedelta(Dod.Date_filter, errors='coerce').dt.days\n",
    "#Dod = Dod.loc[Dod['Date_filter'] < 14].reset_index(drop = True)\n",
    "#create column as Action and fill in \"Delist on depletion\"\n",
    "Dod['Action'] = 'Delist On Depletion'\n",
    "Dod = Dod[['product_id','updated_at','Action']]\n",
    "\n",
    "#filter for Relist product with ids\n",
    "Rel = metadata.copy()\n",
    "Rel = Rel.loc[Rel['updated_field'] == 'segments']\n",
    "Rel = Rel.sort_values(by=\"updated_at\",ascending = False)\n",
    "Rel = Rel.drop_duplicates(subset=['product_id'], keep='first')\n",
    "Rel = Rel.loc[Rel['old_value'] == 7]\n",
    "nv_filter = [6]\n",
    "Rel = Rel[~Rel.new_value.isin(nv_filter)].reset_index(drop = True)\n",
    "Rel['updated_at'] = Rel['updated_at'].dt.date\n",
    "Rel['Date_filter'] = today - Rel['updated_at']\n",
    "Rel['Date_filter'] = pd.to_timedelta(Rel.Date_filter, errors='coerce').dt.days\n",
    "#Rel = Rel.loc[Rel['Date_filter'] < 14].reset_index(drop = True)\n",
    "#create column as Action and fill in \"Relisting\"\n",
    "Rel['Action'] = 'Relisting'\n",
    "Rel = Rel[['product_id','updated_at','Action']]\n",
    "\n",
    "#2nd filter for Relist product with ids\n",
    "Rel2 = metadata.copy()\n",
    "Rel2 = Rel2.loc[Rel2['updated_field'] == 'segments']\n",
    "Rel2 = Rel2.sort_values(by=\"updated_at\",ascending = False)\n",
    "Rel2 = Rel2.drop_duplicates(subset=['product_id'], keep='first')\n",
    "Rel2 = Rel2.loc[Rel2['old_value'] == 6]\n",
    "#Rel2 = Rel2.loc[Rel2['new_value'] == 2]\n",
    "nv_filter = [2]\n",
    "Rel2 = Rel2[Rel2.new_value.isin(nv_filter)].reset_index(drop = True)\n",
    "Rel2['updated_at'] = Rel2['updated_at'].dt.date\n",
    "Rel2['Date_filter'] = today - Rel2['updated_at']\n",
    "Rel2['Date_filter'] = pd.to_timedelta(Rel2.Date_filter, errors='coerce').dt.days\n",
    "#Rel = Rel.loc[Rel['Date_filter'] < 14].reset_index(drop = True)\n",
    "#create column as Action and fill in \"Relisting\"\n",
    "Rel2['Action'] = 'Relisting'\n",
    "Rel2 = Rel2[['product_id','updated_at','Action']]\n",
    "\n",
    "#filter for Delistings product with ids\n",
    "Del = metadata.copy()\n",
    "Del = Del.loc[Del['updated_field'] == 'segments']\n",
    "Del = Del.sort_values(by=\"updated_at\",ascending = False)\n",
    "Del = Del.drop_duplicates(subset=['product_id'], keep='first')\n",
    "#Del = Del.loc[Del['old_value'] == 6]\n",
    "Del = Del.loc[Del['new_value'] == 7]\n",
    "#nv_filter = [2]\n",
    "#Del = Del[Del.new_value.isin(nv_filter)].reset_index(drop = True)\n",
    "Del['updated_at'] = Del['updated_at'].dt.date\n",
    "Del['Date_filter'] = today - Del['updated_at']\n",
    "Del['Date_filter'] = pd.to_timedelta(Del.Date_filter, errors='coerce').dt.days\n",
    "#Rel = Rel.loc[Rel['Date_filter'] < 14].reset_index(drop = True)\n",
    "#create column as Action and fill in \"Relisting\"\n",
    "Del['Action'] = 'Delisted'\n",
    "Del = Del[['product_id','updated_at','Action']]\n",
    "\n",
    "#Concat delist on depletions df and Relisting in one table and format\n",
    "Updates = pd.concat([Dod,Rel, Rel2,Del],ignore_index=True).reset_index(drop=True)\n",
    "Updates = Updates.sort_values(by=\"updated_at\",ascending = False)\n",
    "Updates = Updates.drop_duplicates(subset=['product_id'], keep='first')\n",
    "Updates.rename(columns={'product_id':'Getir Product Id'}, inplace=True)\n",
    "\n",
    "#merge with Main df to add the Action and update date to main df\n",
    "weekly_assortment = pd.merge(Merged_assortment, \n",
    "                      Updates, \n",
    "                      on ='Getir Product Id', \n",
    "                      how ='left')\n",
    "\n",
    "weekly_assortment['Update date'] = weekly_assortment['updated_at']\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "\n",
    "#organising the columns based on requirements\n",
    "weekly_assortment = weekly_assortment[[\\\n",
    "#Update dates\n",
    "'Action',\\\n",
    "'Product Card Creation Date',\\\n",
    "'Update date',\\\n",
    "\n",
    "#ids\n",
    "'Getir Product Id',\\\n",
    "'Eddress Product Id',\\\n",
    "\n",
    "#segments\n",
    "'Created on Eddress',\\\n",
    "'Segment_MDM',\\\n",
    "'Segment_Panel',\\\n",
    "\n",
    "#product statusus on multiple tools\n",
    "'Live Getir app',\\\n",
    "'Live Gorillas app',\\\n",
    "\n",
    "#gtins details\n",
    "'GTIN_Eddress',\\\n",
    "'GTIN_Panel',\\\n",
    "'marketproduct_name_de',\\\n",
    "'brand_name',\\\n",
    "'category_name_en',\\\n",
    "'subcategory_name_en',\\\n",
    "'supplier_names',\\\n",
    "'Buying Price (w VAT)',\\\n",
    "'Selling Price (w VAT)',\\\n",
    "'Buyer',\\\n",
    "\n",
    "\n",
    "#MFCs\n",
    "'Berlin','Berlin mini','Dresden','Leipzig','Hamburg','Bremen','Hannover','Düsseldorf','Dortmund',\\\n",
    "'Köln','Bonn','Essen','München','NürnbergFürth','Frankfurt','Offenbach','Stuttgart','Heidelberg',\\\n",
    "'Mannheim','Karlsruhe','Darmstadt','Augsburg',\\\n",
    "\n",
    "#Cityjoin\n",
    "'Cities',\\\n",
    "#filters\n",
    "'Order_Status',\\\n",
    "    \n",
    "#logistic details\n",
    "'isbundle','exp_days_lifetime','exp_days_allowed','exp_days_warning','exp_days_dead','inbox_quantitiy',\\\n",
    "'palet_inbox_quantity','storagetype','transfer_coli_count','is_picked_to_zero','market_status'\n",
    "]]\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "\n",
    "#Create a ouptut to excel\n",
    "#weekly_assortment.to_excel(\"/Users/swaroopbasavarajmusti/Desktop/weekly_assortment.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bfd0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_G = '/Users/swaroopbasavarajmusti/Desktop/current_stock_info_20230731.xlsx'\n",
    "stock_df = pd.read_excel(Path_G)\n",
    "\n",
    "df = stock_df.copy()\n",
    "\n",
    "df.rename(columns={'product_id':'Getir Product Id'}, inplace=True)\n",
    "\n",
    "Closed_Warehouses = ['Alex','Aplerbeck','Barmbek_Gorillas','Berlin HUB','Bickendorf','Billstedt','Bispingen CW',\\\n",
    "           'Bispingen CW_NoNag','Boeblinger_Gorillas','Cologne - CHOCO','DE_Training_WH','Dortmund HUB - Choco',\\\n",
    "           'Dummy Berlin CW','Friedenau','Gbrunnen Fleet','Getir DE Defleet','Getir DE Office','Getir Germany Test',\\\n",
    "           'Haidhausen','Halensee','Harras','Jungfernheide','JungleTEST','Langwasser','Lichtenberg','Luft_Gorillas',\\\n",
    "           'MagnaPark CW_NoNag','Meringplatz','Munich HUB - Choco','Nordbahnhof','Nürnberg HUB - Choco','Ottensen',\\\n",
    "           'Pasing','Staaken','Stellingen','Tempelhof','Trudering','UNeukolln','Unterbilk','Unterrath','Velodrom',\\\n",
    "           'Wandsbek','Weissensee','Winsviertel','Wittenbergplatz','ZOBMesse','Zabo']\n",
    "for C in Closed_Warehouses:\n",
    "    df = df[df.warehouse_name != C]\n",
    "\n",
    "warehouse_list = df.warehouse_name.unique().tolist()\n",
    "\n",
    "df_1 = df.pivot_table(index=['Getir Product Id'],columns='warehouse_name',values = 'available',aggfunc='sum').\\\n",
    "                reset_index()\n",
    "\n",
    "#df_1.drop(columns = Closed_Warehouses)\n",
    "\n",
    "df_1['Total'] = int(0)\n",
    "for w in warehouse_list:\n",
    "    df_1[w]= df_1.fillna(0)[w].astype(int)\n",
    "    df_1['Total'] = df_1['Total'] + df_1[w]\n",
    "\n",
    "Merged_assortment_stock = pd.merge(Assortment, \n",
    "                      df_1, \n",
    "                      on ='Getir Product Id', \n",
    "                      how ='left')\n",
    "\n",
    "Merged_assortment_stock['Stock_Value'] = Merged_assortment_stock['Total']*\\\n",
    "Merged_assortment_stock['net_net_buying_price_with_vat']\n",
    "Merged_assortment_stock['Stock_Value'] = Merged_assortment_stock['Stock_Value'].fillna(0).astype(int)\n",
    "Merged_assortment_stock['Profit/Product'] = Merged_assortment_stock['g10_price']-Merged_assortment_stock['net_net_buying_price_with_vat']\n",
    "Merged_assortment_stock['Profit/Product'] = round(Merged_assortment_stock['Profit/Product'],2)\n",
    "#cat_wise = Merged_assortment_stock.copy()\n",
    "\n",
    "#cat_wise = cat_wise.groupby(['category_name_en','subcategory_name_en']).sum('Stock_Value').reset_index()\n",
    "#cat_wise = cat_wise.sort_values('Stock_Value', ascending=False).reset_index(drop = True)\n",
    "\n",
    "Path_H = '/Users/swaroopbasavarajmusti/Desktop/marketProductSales.xlsx'\n",
    "Sales_df = pd.read_excel(Path_H)\n",
    "\n",
    "Sales_df.rename(columns={'product_id':'Getir Product Id'}, inplace=True)\n",
    "dict_of_Marketplaces = {k: v for k, v in Sales_df.groupby('integrations_breakdown')}\n",
    "Marketplaces = Sales_df['integrations_breakdown'].unique()\n",
    "Unique_ID_df = pd.DataFrame(Sales_df['Getir Product Id'].unique())\n",
    "Unique_ID_df.rename(columns={0:'Getir Product Id'}, inplace=True)\n",
    "\n",
    "All_means = pd.DataFrame()\n",
    "for M in Marketplaces:\n",
    "    df = dict_of_Marketplaces[M].copy()\n",
    "    df = df.pivot_table(index=['Getir Product Id'],columns='week',values = 'items_sold_count',aggfunc='sum').\\\n",
    "                reset_index()\n",
    "    temp_name = M +\"_mean\"\n",
    "    df[temp_name] = df.mean(numeric_only=True, axis=1)\n",
    "    df[temp_name] = df[temp_name].astype(int)\n",
    "    df = df[['Getir Product Id', temp_name ]]\n",
    "    Unique_ID_df = Unique_ID_df.merge(df, on ='Getir Product Id', how ='left')\n",
    "    \n",
    "Master = pd.merge(Merged_assortment_stock, \n",
    "                  Unique_ID_df, \n",
    "                  on ='Getir Product Id', \n",
    "                  how ='left')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread\n",
    "\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets','https://www.googleapis.com/auth/drive.file', \\\n",
    "        'https://www.googleapis.com/auth/drive','https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('panel-321812-ac5afc8690ea.json',scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "Expiry_data  = client.open('Gsheet trial').worksheet('Stock')\n",
    "Expiry_df = pd.DataFrame(Expiry_data.get_all_records())\n",
    "\n",
    "Expiry_df['Today'] = dt.now().strftime('%Y%m%d')\n",
    "Expiry_df['Today'] = pd.to_datetime(Expiry_df['Today'], errors='coerce') \n",
    "\n",
    "Expiry_df['Expire date'] = pd.to_datetime(Expiry_df['Expire date'], errors='coerce')\n",
    "\n",
    "Expiry_df['Remaining_Days'] = Expiry_df['Expire date'] - Expiry_df['Today']\n",
    "Expiry_df['Remaining_Days'] = pd.to_timedelta(Expiry_df.Remaining_Days, errors='coerce').dt.days\n",
    "\n",
    "Expiry_df = Expiry_df[['Item code','Expire date','Today','Remaining_Days']]\\\n",
    ".sort_values(by = 'Remaining_Days', ascending=[True]).reset_index(drop = True)\n",
    "Expiry_df.rename(columns={'Item code':'Getir Product Id'}, inplace=True)\n",
    "\n",
    "Master_V2 = pd.merge(Master, \n",
    "                  Expiry_df, \n",
    "                  on ='Getir Product Id', \n",
    "                  how ='left')\n",
    "\n",
    "Master_V2 = Master_V2.drop_duplicates(subset=['Getir Product Id', 'Remaining_Days'])\n",
    "\n",
    "Analysis = Master_V2.copy()\n",
    "\n",
    "with pd.ExcelWriter(\"/Users/swaroopbasavarajmusti/Desktop/Promotional_Products_{}.xlsx\".format(dte)) as writer:\n",
    "    for M in Marketplaces:\n",
    "        Analysis['Cover_days_'+M] = (Analysis['Total']/Analysis[M+'_mean'])*7\n",
    "        Analysis['Cover_days_'+M] = Analysis['Cover_days_'+M].fillna(0).astype(int)\n",
    "        Analysis['STR_'+M] = round((Analysis[M+'_mean']/Analysis['Total'])*100, 2)\n",
    "        Analysis['Risk_Rank_'+M] = round((Analysis['Cover_days_'+M] - Analysis['Remaining_Days']), 0).rank(ascending=False)\n",
    "        Analysis['Risk_Rank_'+M] = Analysis['Risk_Rank_'+M].fillna(0).astype(int)\n",
    "        \n",
    "        temp_df = Analysis.copy()\n",
    "        temp_df =  temp_df[(temp_df['Cover_days_'+M]>60)]\n",
    "        temp_df = temp_df[['createdatl','Getir Product Id','product_barcode','marketproduct_name_de',\\\n",
    "                          'category_name_en','subcategory_name_en','segment_name','Total','Stock_Value','Profit/Product',\\\n",
    "                          M+'_mean','STR_'+M,'Cover_days_'+M,'Expire date','Remaining_Days','Risk_Rank_'+M]].sort_values(by=\"Cover_days_\"+M,ascending = False).reset_index(drop = True)\n",
    "        temp_df = temp_df.sort_values(by='Risk_Rank_'+M,ascending = True).reset_index(drop = True)\n",
    "        temp_df.to_excel(writer, sheet_name = M)\n",
    "        col = \"Risk_Rank_\"+M\n",
    "        temp_df_2 = temp_df[(temp_df[col] > 0)& (temp_df[col] < 31)].reset_index(drop = True)\n",
    "        temp_df_2.to_excel(writer, sheet_name = \"top_30_\"+M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0d3d6",
   "metadata": {},
   "source": [
    "Gorillas Only Products: Private Label Products: Indy started a list here - let’s use some kind of dictionary like that: https://docs.google.com/spreadsheets/d/1HbBSPP495FKP2syAOkbMEPOGrlFozz0LzI2T0H1mV44/edit#gid=979825498\n",
    "Getir only products: Define List (Chris)\n",
    "Columns to add (middterm):\n",
    "Local/Regional/National products: define rule depending on transfer groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
